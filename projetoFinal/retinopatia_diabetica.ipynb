{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifica√ß√£o de Retinopatia Diab√©tica\n",
    "\n",
    "## Objetivo\n",
    "Desenvolver um modelo de classifica√ß√£o bin√°ria para detectar a presen√ßa de retinopatia diab√©tica em imagens de retina.\n",
    "\n",
    "## Hip√≥tese\n",
    "\"A diferen√ßa de tons gerais - do mais claro ao mais escuro - e presen√ßa de diferentes tons, indica a presen√ßa da retinopatia.\"\n",
    "\n",
    "## Dataset\n",
    "- **Saud√°veis (No DR)**: 525 imagens\n",
    "- **Doentes**: 1461 imagens (Mild: 370, Moderate: 599, Proliferate: 290, Severe: 202)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instala√ß√£o e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o das depend√™ncias\n",
    "!pip install kagglehub tensorflow scikit-learn matplotlib seaborn opencv-python pillow tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import ResNet50, VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU dispon√≠vel: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download do dataset\n",
    "path = kagglehub.dataset_download(\"jockeroika/diabetic-retinopathy\")\n",
    "print(\"Caminho do dataset:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorar estrutura do dataset\n",
    "import os\n",
    "\n",
    "def explore_directory(path, indent=0):\n",
    "    \"\"\"Explora e mostra a estrutura de diret√≥rios\"\"\"\n",
    "    items = os.listdir(path)\n",
    "    for item in items:\n",
    "        item_path = os.path.join(path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            files = len([f for f in os.listdir(item_path) if os.path.isfile(os.path.join(item_path, f))])\n",
    "            print(\" \" * indent + f\"üìÅ {item}/ ({files} arquivos)\")\n",
    "            if indent < 2:  # Limitar profundidade\n",
    "                explore_directory(item_path, indent + 2)\n",
    "        else:\n",
    "            print(\" \" * indent + f\"üìÑ {item}\")\n",
    "\n",
    "explore_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carregamento e Explora√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configura√ß√µes\nIMG_SIZE = 128  # Tamanho para redimensionar as imagens\nRANDOM_STATE = 42\n\n# Mapear classes para bin√°rio: 0 = Saud√°vel, 1 = Doente\n# Nomes das pastas no dataset: 'Healthy', 'Mild DR', 'Moderate DR', 'Proliferate DR', 'Severe DR'\nCLASS_MAPPING = {\n    'Healthy': 0,         # Saud√°vel\n    'Mild': 1,            # Doente (Mild DR)\n    'Moderate': 1,        # Doente (Moderate DR)\n    'Proliferate': 1,     # Doente (Proliferate DR)\n    'Severe': 1           # Doente (Severe DR)\n}\n\nprint(\"Mapeamento de classes:\")\nprint(\"0 = Saud√°vel (Healthy)\")\nprint(\"1 = Doente (Mild DR, Moderate DR, Proliferate DR, Severe DR)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(base_path, class_mapping, img_size=128):\n",
    "    \"\"\"\n",
    "    Carrega imagens e converte para classifica√ß√£o bin√°ria.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    original_labels = []  # Para an√°lise\n",
    "    \n",
    "    # Procurar por subpastas (train/test ou diretamente as classes)\n",
    "    possible_paths = [\n",
    "        base_path,\n",
    "        os.path.join(base_path, 'train'),\n",
    "        os.path.join(base_path, 'gaussian_filtered_images'),\n",
    "        os.path.join(base_path, 'gaussian_filtered_images', 'gaussian_filtered_images')\n",
    "    ]\n",
    "    \n",
    "    data_path = None\n",
    "    for p in possible_paths:\n",
    "        if os.path.exists(p):\n",
    "            subdirs = [d for d in os.listdir(p) if os.path.isdir(os.path.join(p, d))]\n",
    "            if any(key.lower() in d.lower() for d in subdirs for key in class_mapping.keys()):\n",
    "                data_path = p\n",
    "                break\n",
    "    \n",
    "    if data_path is None:\n",
    "        print(\"Estrutura de diret√≥rios encontrada:\")\n",
    "        explore_directory(base_path)\n",
    "        raise ValueError(\"N√£o foi poss√≠vel encontrar as pastas de classes. Verifique a estrutura.\")\n",
    "    \n",
    "    print(f\"Carregando imagens de: {data_path}\")\n",
    "    \n",
    "    # Iterar sobre as classes\n",
    "    for folder_name in os.listdir(data_path):\n",
    "        folder_path = os.path.join(data_path, folder_name)\n",
    "        \n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        \n",
    "        # Encontrar a classe correspondente\n",
    "        binary_label = None\n",
    "        for class_name, label in class_mapping.items():\n",
    "            if class_name.lower() in folder_name.lower():\n",
    "                binary_label = label\n",
    "                break\n",
    "        \n",
    "        if binary_label is None:\n",
    "            print(f\"Pasta '{folder_name}' n√£o mapeada, pulando...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processando: {folder_name} -> {'Saud√°vel' if binary_label == 0 else 'Doente'}\")\n",
    "        \n",
    "        # Carregar imagens da pasta\n",
    "        image_files = [f for f in os.listdir(folder_path) \n",
    "                      if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp'))]\n",
    "        \n",
    "        for img_name in tqdm(image_files, desc=folder_name):\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            try:\n",
    "                # Carregar e processar imagem\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    continue\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = cv2.resize(img, (img_size, img_size))\n",
    "                \n",
    "                images.append(img)\n",
    "                labels.append(binary_label)\n",
    "                original_labels.append(folder_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao carregar {img_path}: {e}\")\n",
    "    \n",
    "    return np.array(images), np.array(labels), original_labels\n",
    "\n",
    "# Carregar dados\n",
    "X, y, original_labels = load_images_from_folder(path, CLASS_MAPPING, IMG_SIZE)\n",
    "print(f\"\\nTotal de imagens carregadas: {len(X)}\")\n",
    "print(f\"Shape das imagens: {X.shape}\")\n",
    "print(f\"Distribui√ß√£o: Saud√°veis={np.sum(y==0)}, Doentes={np.sum(y==1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar distribui√ß√£o das classes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribui√ß√£o bin√°ria\n",
    "class_counts = pd.Series(y).value_counts().sort_index()\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "axes[0].bar(['Saud√°vel (0)', 'Doente (1)'], class_counts.values, color=colors)\n",
    "axes[0].set_title('Distribui√ß√£o das Classes (Bin√°ria)')\n",
    "axes[0].set_ylabel('Quantidade')\n",
    "for i, v in enumerate(class_counts.values):\n",
    "    axes[0].text(i, v + 10, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Distribui√ß√£o original\n",
    "orig_counts = pd.Series(original_labels).value_counts()\n",
    "axes[1].bar(range(len(orig_counts)), orig_counts.values, color=plt.cm.viridis(np.linspace(0, 1, len(orig_counts))))\n",
    "axes[1].set_xticks(range(len(orig_counts)))\n",
    "axes[1].set_xticklabels(orig_counts.index, rotation=45, ha='right')\n",
    "axes[1].set_title('Distribui√ß√£o Original das Classes')\n",
    "axes[1].set_ylabel('Quantidade')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distribuicao_classes.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPropor√ß√£o: {np.sum(y==0)/len(y)*100:.1f}% Saud√°veis, {np.sum(y==1)/len(y)*100:.1f}% Doentes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar exemplos de imagens\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "# Imagens saud√°veis\n",
    "healthy_idx = np.where(y == 0)[0]\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    idx = healthy_idx[i]\n",
    "    ax.imshow(X[idx])\n",
    "    ax.set_title('Saud√°vel', color='green')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Imagens com retinopatia\n",
    "sick_idx = np.where(y == 1)[0]\n",
    "for i, ax in enumerate(axes[1]):\n",
    "    idx = sick_idx[i]\n",
    "    ax.imshow(X[idx])\n",
    "    ax.set_title('Retinopatia', color='red')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Exemplos de Imagens de Retina', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('exemplos_imagens.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pr√©-processamento\n",
    "\n",
    "### An√°lise de tons (baseado na hip√≥tese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tone_features(images):\n",
    "    \"\"\"\n",
    "    Extrai caracter√≠sticas de tons das imagens para validar a hip√≥tese.\n",
    "    - M√©dia e desvio padr√£o dos canais RGB\n",
    "    - Histograma de intensidades\n",
    "    - Contraste\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for img in tqdm(images, desc=\"Extraindo features de tons\"):\n",
    "        # Converter para escala de cinza para an√°lise de tons\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Features b√°sicas\n",
    "        feat = {\n",
    "            'mean_intensity': np.mean(gray),\n",
    "            'std_intensity': np.std(gray),\n",
    "            'min_intensity': np.min(gray),\n",
    "            'max_intensity': np.max(gray),\n",
    "            'contrast': np.max(gray) - np.min(gray),\n",
    "            'mean_r': np.mean(img[:,:,0]),\n",
    "            'mean_g': np.mean(img[:,:,1]),\n",
    "            'mean_b': np.mean(img[:,:,2]),\n",
    "            'std_r': np.std(img[:,:,0]),\n",
    "            'std_g': np.std(img[:,:,1]),\n",
    "            'std_b': np.std(img[:,:,2]),\n",
    "        }\n",
    "        \n",
    "        # Histograma em quartis\n",
    "        hist, _ = np.histogram(gray, bins=4, range=(0, 256))\n",
    "        hist = hist / hist.sum()  # Normalizar\n",
    "        feat['hist_q1'] = hist[0]  # Tons escuros\n",
    "        feat['hist_q2'] = hist[1]\n",
    "        feat['hist_q3'] = hist[2]\n",
    "        feat['hist_q4'] = hist[3]  # Tons claros\n",
    "        \n",
    "        features.append(feat)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "# Extrair features de tons\n",
    "tone_features = extract_tone_features(X)\n",
    "tone_features['label'] = y\n",
    "print(tone_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar diferen√ßas de tons entre classes\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "metrics = ['mean_intensity', 'std_intensity', 'contrast', 'mean_r', 'mean_g', 'mean_b']\n",
    "titles = ['Intensidade M√©dia', 'Desvio Padr√£o', 'Contraste', 'M√©dia Canal R', 'M√©dia Canal G', 'M√©dia Canal B']\n",
    "\n",
    "for ax, metric, title in zip(axes.flat, metrics, titles):\n",
    "    healthy_data = tone_features[tone_features['label'] == 0][metric]\n",
    "    sick_data = tone_features[tone_features['label'] == 1][metric]\n",
    "    \n",
    "    ax.boxplot([healthy_data, sick_data], labels=['Saud√°vel', 'Doente'])\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Valor')\n",
    "\n",
    "plt.suptitle('An√°lise de Tons por Classe (Hip√≥tese)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('analise_tons.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Estat√≠sticas descritivas por classe\n",
    "print(\"\\n=== Estat√≠sticas por Classe ===\")\n",
    "print(tone_features.groupby('label')[metrics].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza√ß√£o das imagens para os modelos\n",
    "X_normalized = X.astype('float32') / 255.0\n",
    "\n",
    "# Divis√£o treino/teste (80/20) com estratifica√ß√£o\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Treino: {X_train.shape[0]} imagens\")\n",
    "print(f\"Teste: {X_test.shape[0]} imagens\")\n",
    "print(f\"\\nDistribui√ß√£o no treino: Saud√°veis={np.sum(y_train==0)}, Doentes={np.sum(y_train==1)}\")\n",
    "print(f\"Distribui√ß√£o no teste: Saud√°veis={np.sum(y_test==0)}, Doentes={np.sum(y_test==1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation para o treinamento\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n",
    "print(\"Data augmentation configurado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelo Simples - MLP (Multi-Layer Perceptron)\n",
    "\n",
    "Usaremos as features extra√≠das baseadas em tons + features flattened da imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para MLP (flatten + features de tons)\n",
    "X_flat_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_flat_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normaliza√ß√£o com StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_flat_train_scaled = scaler.fit_transform(X_flat_train)\n",
    "X_flat_test_scaled = scaler.transform(X_flat_test)\n",
    "\n",
    "print(f\"Shape para MLP: {X_flat_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo MLP simples\n",
    "def create_mlp_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(input_shape,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "mlp_model = create_mlp_model(X_flat_train_scaled.shape[1])\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, \n",
    "    patience=5, \n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Calcular class weights para lidar com desbalanceamento\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "print(f\"Class weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar MLP\n",
    "print(\"=\" * 50)\n",
    "print(\"TREINANDO MODELO MLP (SIMPLES)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "history_mlp = mlp_model.fit(\n",
    "    X_flat_train_scaled, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar MLP\n",
    "y_pred_mlp_prob = mlp_model.predict(X_flat_test_scaled)\n",
    "y_pred_mlp = (y_pred_mlp_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RESULTADOS - MODELO MLP (SIMPLES)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nRelat√≥rio de Classifica√ß√£o:\")\n",
    "print(classification_report(y_test, y_pred_mlp, target_names=['Saud√°vel', 'Doente']))\n",
    "\n",
    "print(f\"\\nAcur√°cia: {accuracy_score(y_test, y_pred_mlp):.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred_mlp_prob):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modelo Avan√ßado - CNN com Transfer Learning (ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo CNN avan√ßado com Transfer Learning\n",
    "def create_cnn_advanced(input_shape):\n",
    "    # Usar ResNet50 pr√©-treinada\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Congelar as camadas base inicialmente\n",
    "    for layer in base_model.layers[:-20]:  # Descongelar √∫ltimas 20 camadas\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Adicionar camadas de classifica√ß√£o\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "cnn_model = create_cnn_advanced((IMG_SIZE, IMG_SIZE, 3))\n",
    "print(f\"\\nTotal de par√¢metros: {cnn_model.count_params():,}\")\n",
    "print(f\"Par√¢metros trein√°veis: {sum([tf.keras.backend.count_params(w) for w in cnn_model.trainable_weights]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar CNN avan√ßada\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TREINANDO MODELO CNN AVAN√áADO (ResNet50)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "early_stop_cnn = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=15, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr_cnn = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, \n",
    "    patience=5, \n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "history_cnn = cnn_model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=32),\n",
    "    epochs=30,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop_cnn, reduce_lr_cnn],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar CNN\n",
    "y_pred_cnn_prob = cnn_model.predict(X_test)\n",
    "y_pred_cnn = (y_pred_cnn_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RESULTADOS - MODELO CNN AVAN√áADO (ResNet50)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nRelat√≥rio de Classifica√ß√£o:\")\n",
    "print(classification_report(y_test, y_pred_cnn, target_names=['Saud√°vel', 'Doente']))\n",
    "\n",
    "print(f\"\\nAcur√°cia: {accuracy_score(y_test, y_pred_cnn):.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred_cnn_prob):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compara√ß√£o dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para plotar matriz de confus√£o\n",
    "def plot_confusion_matrix(y_true, y_pred, title, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Saud√°vel', 'Doente'],\n",
    "                yticklabels=['Saud√°vel', 'Doente'])\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Real')\n",
    "    ax.set_xlabel('Predito')\n",
    "\n",
    "# Comparar matrizes de confus√£o\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_mlp, 'MLP (Modelo Simples)', axes[0])\n",
    "plot_confusion_matrix(y_test, y_pred_cnn, 'CNN ResNet50 (Modelo Avan√ßado)', axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('matrizes_confusao.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas ROC\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# MLP\n",
    "fpr_mlp, tpr_mlp, _ = roc_curve(y_test, y_pred_mlp_prob)\n",
    "auc_mlp = roc_auc_score(y_test, y_pred_mlp_prob)\n",
    "ax.plot(fpr_mlp, tpr_mlp, label=f'MLP (AUC = {auc_mlp:.3f})', linewidth=2)\n",
    "\n",
    "# CNN\n",
    "fpr_cnn, tpr_cnn, _ = roc_curve(y_test, y_pred_cnn_prob)\n",
    "auc_cnn = roc_auc_score(y_test, y_pred_cnn_prob)\n",
    "ax.plot(fpr_cnn, tpr_cnn, label=f'CNN ResNet50 (AUC = {auc_cnn:.3f})', linewidth=2)\n",
    "\n",
    "# Linha diagonal (classificador aleat√≥rio)\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Aleat√≥rio (AUC = 0.5)')\n",
    "\n",
    "ax.set_xlabel('Taxa de Falsos Positivos')\n",
    "ax.set_ylabel('Taxa de Verdadeiros Positivos')\n",
    "ax.set_title('Curvas ROC - Compara√ß√£o dos Modelos')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.savefig('curvas_roc.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hist√≥rico de treinamento\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# MLP - Loss\n",
    "axes[0, 0].plot(history_mlp.history['loss'], label='Treino')\n",
    "axes[0, 0].plot(history_mlp.history['val_loss'], label='Valida√ß√£o')\n",
    "axes[0, 0].set_title('MLP - Loss')\n",
    "axes[0, 0].set_xlabel('√âpoca')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# MLP - Accuracy\n",
    "axes[0, 1].plot(history_mlp.history['accuracy'], label='Treino')\n",
    "axes[0, 1].plot(history_mlp.history['val_accuracy'], label='Valida√ß√£o')\n",
    "axes[0, 1].set_title('MLP - Acur√°cia')\n",
    "axes[0, 1].set_xlabel('√âpoca')\n",
    "axes[0, 1].set_ylabel('Acur√°cia')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# CNN - Loss\n",
    "axes[1, 0].plot(history_cnn.history['loss'], label='Treino')\n",
    "axes[1, 0].plot(history_cnn.history['val_loss'], label='Valida√ß√£o')\n",
    "axes[1, 0].set_title('CNN ResNet50 - Loss')\n",
    "axes[1, 0].set_xlabel('√âpoca')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# CNN - Accuracy\n",
    "axes[1, 1].plot(history_cnn.history['accuracy'], label='Treino')\n",
    "axes[1, 1].plot(history_cnn.history['val_accuracy'], label='Valida√ß√£o')\n",
    "axes[1, 1].set_title('CNN ResNet50 - Acur√°cia')\n",
    "axes[1, 1].set_xlabel('√âpoca')\n",
    "axes[1, 1].set_ylabel('Acur√°cia')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('historico_treinamento.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela comparativa\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "results = {\n",
    "    'M√©trica': ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score', 'AUC-ROC'],\n",
    "    'MLP (Simples)': [\n",
    "        accuracy_score(y_test, y_pred_mlp),\n",
    "        precision_score(y_test, y_pred_mlp),\n",
    "        recall_score(y_test, y_pred_mlp),\n",
    "        f1_score(y_test, y_pred_mlp),\n",
    "        roc_auc_score(y_test, y_pred_mlp_prob)\n",
    "    ],\n",
    "    'CNN ResNet50 (Avan√ßado)': [\n",
    "        accuracy_score(y_test, y_pred_cnn),\n",
    "        precision_score(y_test, y_pred_cnn),\n",
    "        recall_score(y_test, y_pred_cnn),\n",
    "        f1_score(y_test, y_pred_cnn),\n",
    "        roc_auc_score(y_test, y_pred_cnn_prob)\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results['MLP (Simples)'] = df_results['MLP (Simples)'].apply(lambda x: f'{x:.4f}')\n",
    "df_results['CNN ResNet50 (Avan√ßado)'] = df_results['CNN ResNet50 (Avan√ßado)'].apply(lambda x: f'{x:.4f}')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARA√á√ÉO FINAL DOS MODELOS\")\n",
    "print(\"=\" * 60)\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Valida√ß√£o Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valida√ß√£o Cruzada com K-Fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"VALIDA√á√ÉO CRUZADA ({n_folds}-FOLD)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Resultados para cada fold\n",
    "mlp_cv_scores = []\n",
    "cnn_cv_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_normalized, y), 1):\n",
    "    print(f\"\\n--- Fold {fold}/{n_folds} ---\")\n",
    "    \n",
    "    # Dividir dados\n",
    "    X_cv_train, X_cv_val = X_normalized[train_idx], X_normalized[val_idx]\n",
    "    y_cv_train, y_cv_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # MLP\n",
    "    X_cv_train_flat = X_cv_train.reshape(X_cv_train.shape[0], -1)\n",
    "    X_cv_val_flat = X_cv_val.reshape(X_cv_val.shape[0], -1)\n",
    "    \n",
    "    scaler_cv = StandardScaler()\n",
    "    X_cv_train_scaled = scaler_cv.fit_transform(X_cv_train_flat)\n",
    "    X_cv_val_scaled = scaler_cv.transform(X_cv_val_flat)\n",
    "    \n",
    "    mlp_cv = create_mlp_model(X_cv_train_scaled.shape[1])\n",
    "    mlp_cv.fit(\n",
    "        X_cv_train_scaled, y_cv_train,\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    mlp_cv_pred = (mlp_cv.predict(X_cv_val_scaled) > 0.5).astype(int).flatten()\n",
    "    mlp_cv_scores.append(accuracy_score(y_cv_val, mlp_cv_pred))\n",
    "    print(f\"  MLP Acur√°cia: {mlp_cv_scores[-1]:.4f}\")\n",
    "    \n",
    "    # CNN (simplificada para CV ser mais r√°pida)\n",
    "    cnn_cv = create_cnn_advanced((IMG_SIZE, IMG_SIZE, 3))\n",
    "    cnn_cv.fit(\n",
    "        X_cv_train, y_cv_train,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=[EarlyStopping(patience=3, restore_best_weights=True)],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    cnn_cv_pred = (cnn_cv.predict(X_cv_val) > 0.5).astype(int).flatten()\n",
    "    cnn_cv_scores.append(accuracy_score(y_cv_val, cnn_cv_pred))\n",
    "    print(f\"  CNN Acur√°cia: {cnn_cv_scores[-1]:.4f}\")\n",
    "    \n",
    "    # Limpar mem√≥ria\n",
    "    del mlp_cv, cnn_cv\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados da Valida√ß√£o Cruzada\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESULTADOS DA VALIDA√á√ÉO CRUZADA\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nMLP (Modelo Simples):\")\n",
    "print(f\"  Scores por fold: {[f'{s:.4f}' for s in mlp_cv_scores]}\")\n",
    "print(f\"  M√©dia: {np.mean(mlp_cv_scores):.4f} (+/- {np.std(mlp_cv_scores)*2:.4f})\")\n",
    "\n",
    "print(f\"\\nCNN ResNet50 (Modelo Avan√ßado):\")\n",
    "print(f\"  Scores por fold: {[f'{s:.4f}' for s in cnn_cv_scores]}\")\n",
    "print(f\"  M√©dia: {np.mean(cnn_cv_scores):.4f} (+/- {np.std(cnn_cv_scores)*2:.4f})\")\n",
    "\n",
    "# Visualizar resultados CV\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(n_folds)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, mlp_cv_scores, width, label='MLP', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, cnn_cv_scores, width, label='CNN ResNet50', color='coral')\n",
    "\n",
    "ax.axhline(y=np.mean(mlp_cv_scores), color='steelblue', linestyle='--', alpha=0.7, label=f'MLP M√©dia: {np.mean(mlp_cv_scores):.4f}')\n",
    "ax.axhline(y=np.mean(cnn_cv_scores), color='coral', linestyle='--', alpha=0.7, label=f'CNN M√©dia: {np.mean(cnn_cv_scores):.4f}')\n",
    "\n",
    "ax.set_xlabel('Fold')\n",
    "ax.set_ylabel('Acur√°cia')\n",
    "ax.set_title('Valida√ß√£o Cruzada - Compara√ß√£o dos Modelos')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'Fold {i+1}' for i in range(n_folds)])\n",
    "ax.legend()\n",
    "ax.set_ylim([0.5, 1.0])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('validacao_cruzada.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclus√µes e Relat√≥rio Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RELAT√ìRIO FINAL - CLASSIFICA√á√ÉO DE RETINOPATIA DIAB√âTICA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä DATASET:\")\n",
    "print(f\"   - Total de imagens: {len(X)}\")\n",
    "print(f\"   - Saud√°veis: {np.sum(y==0)} ({np.sum(y==0)/len(y)*100:.1f}%)\")\n",
    "print(f\"   - Doentes: {np.sum(y==1)} ({np.sum(y==1)/len(y)*100:.1f}%)\")\n",
    "print(f\"   - Tamanho das imagens: {IMG_SIZE}x{IMG_SIZE} pixels\")\n",
    "\n",
    "print(\"\\nüî¨ HIP√ìTESE:\")\n",
    "print(\"   'A diferen√ßa de tons gerais - do mais claro ao mais escuro - \")\n",
    "print(\"    e presen√ßa de diferentes tons, indica a presen√ßa da retinopatia.'\")\n",
    "\n",
    "print(\"\\nüìà RESULTADOS DOS MODELOS:\")\n",
    "print(\"\\n   Modelo MLP (Simples):\")\n",
    "print(f\"      - Acur√°cia no teste: {accuracy_score(y_test, y_pred_mlp):.4f}\")\n",
    "print(f\"      - AUC-ROC: {roc_auc_score(y_test, y_pred_mlp_prob):.4f}\")\n",
    "print(f\"      - Valida√ß√£o Cruzada: {np.mean(mlp_cv_scores):.4f} (+/- {np.std(mlp_cv_scores)*2:.4f})\")\n",
    "\n",
    "print(\"\\n   Modelo CNN ResNet50 (Avan√ßado):\")\n",
    "print(f\"      - Acur√°cia no teste: {accuracy_score(y_test, y_pred_cnn):.4f}\")\n",
    "print(f\"      - AUC-ROC: {roc_auc_score(y_test, y_pred_cnn_prob):.4f}\")\n",
    "print(f\"      - Valida√ß√£o Cruzada: {np.mean(cnn_cv_scores):.4f} (+/- {np.std(cnn_cv_scores)*2:.4f})\")\n",
    "\n",
    "print(\"\\nüèÜ MELHOR MODELO:\")\n",
    "if np.mean(cnn_cv_scores) > np.mean(mlp_cv_scores):\n",
    "    print(\"   CNN ResNet50 (Modelo Avan√ßado)\")\n",
    "    print(f\"   Melhoria sobre MLP: {(np.mean(cnn_cv_scores) - np.mean(mlp_cv_scores))*100:.2f}%\")\n",
    "else:\n",
    "    print(\"   MLP (Modelo Simples)\")\n",
    "    print(f\"   Melhoria sobre CNN: {(np.mean(mlp_cv_scores) - np.mean(cnn_cv_scores))*100:.2f}%\")\n",
    "\n",
    "print(\"\\nüìÅ ARQUIVOS GERADOS:\")\n",
    "print(\"   - distribuicao_classes.png\")\n",
    "print(\"   - exemplos_imagens.png\")\n",
    "print(\"   - analise_tons.png\")\n",
    "print(\"   - matrizes_confusao.png\")\n",
    "print(\"   - curvas_roc.png\")\n",
    "print(\"   - historico_treinamento.png\")\n",
    "print(\"   - validacao_cruzada.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelos\n",
    "mlp_model.save('modelo_mlp_retinopatia.h5')\n",
    "cnn_model.save('modelo_cnn_resnet50_retinopatia.h5')\n",
    "print(\"Modelos salvos com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}